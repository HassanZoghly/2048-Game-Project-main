{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div style=\"background: linear-gradient(135deg,rgb(65, 202, 179),rgb(201, 224, 96)); \n",
    "        color:rgb(0, 0, 0); \n",
    "        width: 100%; \n",
    "        height: 60px; \n",
    "        text-align: center; \n",
    "        font-weight: bold; \n",
    "        line-height: 60px; \n",
    "        margin: 2 px 0; \n",
    "        font-size: 44px; \n",
    "        border-radius: 10px; \n",
    "        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);\">\n",
    "    2048 Deep Reinforcement Learning\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Python Libraries\n",
    "import math\n",
    "import random \n",
    "from copy import deepcopy\n",
    "\n",
    "# Third-Party Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### **Game Logic**\n",
    "\n",
    "<h3 align=\"center\"><strong>Create a game and add Tiles</strong></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new 4x4 game board filled with zeros.\n",
    "def new_game(n):\n",
    "    matrix = np.zeros([n,n])\n",
    "    return matrix\n",
    "\n",
    "# Add a 2 (90%) or 4 (10%) tile to a random empty cell in a 4x4 matrix.\n",
    "def add_two(mat):\n",
    "    empty_cells = []\n",
    "    for i in range(len(mat)):\n",
    "        for j in range(len(mat[0])):\n",
    "            if(mat[i][j]==0):\n",
    "                empty_cells.append((i,j))\n",
    "    if(len(empty_cells)==0):\n",
    "        return mat\n",
    "    \n",
    "    index_pair = empty_cells[random.randint(0,len(empty_cells)-1)]\n",
    "    \n",
    "    prob = random.random()\n",
    "    if(prob>=0.9):\n",
    "        mat[index_pair[0]][index_pair[1]]=4\n",
    "    else:\n",
    "        mat[index_pair[0]][index_pair[1]]=2\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\"><strong>Checking the game status</strong></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the 4x4 game is over or ongoing.\n",
    "def game_state(mat):\n",
    "    # if 2048 in mat:\n",
    "    #     return 'win'\n",
    "    \n",
    "    for i in range(len(mat)-1):         # intentionally reduced to check the row on the right and below\n",
    "        for j in range(len(mat[0])-1):  # more elegant to use exceptions but most likely this will be their solution\n",
    "            if mat[i][j]==mat[i+1][j] or mat[i][j+1]==mat[i][j]:\n",
    "                return 'not over'\n",
    "            \n",
    "    for i in range(len(mat)):           # check for any zero entries\n",
    "        for j in range(len(mat[0])):\n",
    "            if mat[i][j]==0:\n",
    "                return 'not over'\n",
    "            \n",
    "    for k in range(len(mat)-1):         # to check the left/right entries on the last row\n",
    "        if mat[len(mat)-1][k]==mat[len(mat)-1][k+1]:\n",
    "            return 'not over'\n",
    "        \n",
    "    for j in range(len(mat)-1):         # check up/down entries on last column\n",
    "        if mat[j][len(mat)-1]==mat[j+1][len(mat)-1]:\n",
    "            return 'not over'\n",
    "        \n",
    "    return 'lose'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\"><strong>helper functions for movements</strong></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse each row of a 4x4 matrix (left to right).\n",
    "def reverse(mat):\n",
    "    new=[]\n",
    "    for i in range(len(mat)):\n",
    "        new.append([])\n",
    "        for j in range(len(mat[0])):\n",
    "            new[i].append(mat[i][len(mat[0])-j-1])\n",
    "    return new\n",
    "\n",
    "# Transpose the 4x4 matrix.\n",
    "def transpose(mat):\n",
    "    new=[]\n",
    "    for i in range(len(mat[0])):\n",
    "        new.append([])\n",
    "        for j in range(len(mat)):\n",
    "            new[i].append(mat[j][i])\n",
    "            \n",
    "    return np.transpose(mat)\n",
    "\n",
    "# Move non-zero tiles to the left in 4x4 board\n",
    "def cover_up(mat):\n",
    "    new = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
    "    done = False\n",
    "    for i in range(4):\n",
    "        count = 0\n",
    "        for j in range(4):\n",
    "            if mat[i][j]!=0:\n",
    "                new[i][count] = mat[i][j]\n",
    "                if j!=count:\n",
    "                    done=True\n",
    "                count+=1\n",
    "    return (new,done)\n",
    "\n",
    "# Merge equal adjacent tiles in 4x4 board\n",
    "def merge(mat):\n",
    "    done=False\n",
    "    score = 0\n",
    "    for i in range(4):\n",
    "        for j in range(3):\n",
    "            if mat[i][j]==mat[i][j+1] and mat[i][j]!=0:\n",
    "                mat[i][j]*=2\n",
    "                score += mat[i][j]   \n",
    "                mat[i][j+1]=0\n",
    "                done=True\n",
    "    return (mat,done,score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\"><strong>Functions of movements</strong></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up(game):\n",
    "        game = transpose(game)\n",
    "        game,done = cover_up(game)\n",
    "        temp = merge(game)\n",
    "        game = temp[0]\n",
    "        done = done or temp[1]\n",
    "        game = cover_up(game)[0]\n",
    "        game = transpose(game)\n",
    "        return (game,done,temp[2])\n",
    "\n",
    "def down(game):\n",
    "        game=reverse(transpose(game))\n",
    "        game,done=cover_up(game)\n",
    "        temp=merge(game)\n",
    "        game=temp[0]\n",
    "        done=done or temp[1]\n",
    "        game=cover_up(game)[0]\n",
    "        game=transpose(reverse(game))\n",
    "        return (game,done,temp[2])\n",
    "\n",
    "def left(game):\n",
    "        game,done=cover_up(game)\n",
    "        temp=merge(game)\n",
    "        game=temp[0]\n",
    "        done=done or temp[1]\n",
    "        game=cover_up(game)[0]\n",
    "        return (game,done,temp[2])\n",
    "\n",
    "def right(game):\n",
    "        game=reverse(game)\n",
    "        game,done=cover_up(game)\n",
    "        temp=merge(game)\n",
    "        game=temp[0]\n",
    "        done=done or temp[1]\n",
    "        game=cover_up(game)[0]\n",
    "        game=reverse(game)\n",
    "        return (game,done,temp[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\"><strong>State Processing Functions</strong></h3>\n",
    "\n",
    "* Important Functions\n",
    "    * Find Empty Cell Function (Used in Reward)\n",
    "    * Convert Input Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the number of empty cells in the game matrix.\n",
    "def findemptyCell(mat):\n",
    "    count = 0\n",
    "    for i in range(len(mat)):\n",
    "        for j in range(len(mat)):\n",
    "            if(mat[i][j]==0):\n",
    "                count+=1\n",
    "    return count\n",
    "\n",
    "# convert the input game matrix into corresponding power of 2 matrix.\n",
    "def change_values(X):\n",
    "    power_mat = np.zeros(shape=(1,4,4,16),dtype=np.float32)\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            if(X[i][j]==0):\n",
    "                power_mat[0][i][j][0] = 1.0\n",
    "            else:\n",
    "                power = int(math.log(X[i][j],2))\n",
    "                power_mat[0][i][j][power] = 1.0\n",
    "    return power_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controls = {0:up,1:left,2:right,3:down}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\"><strong>Hyper Parameters & Network Architecture</strong></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_learning_rate = 0.0005\n",
    "gamma = 0.9                     # gamma for Q-learning\n",
    "epsilon = 0.9                   # epsilon for epsilon-greedy strategy\n",
    "replay_memory = []              # states of the game : to store states and labels of the game for training\n",
    "replay_labels = []              # labels of the states\n",
    "mem_capacity = 6000             # capacity of the replay memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Network Architecture\n",
    "> ![](https://github.com/navjindervirdee/2048-deep-reinforcement-learning/blob/master/Architecture/Architecture.JPG?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth1 = 128            # first convolution layer depth\n",
    "depth2 = 128            # second convolution layer depth\n",
    "batch_size   = 512      # batch size for batch gradient descent\n",
    "input_units  = 16       # input units\n",
    "hidden_units = 256      # fully connected layer neurons\n",
    "output_units = 4        # output neurons = number of moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### **Let's make the Tensorflow Graph** &  **Define the DQN model**\n",
    ">* Loss ℒ  = mean[(Q(sₜ,aₜ) - (r + γ·maxₐQ(sₜ₊₁,a)))²]\n",
    "> \n",
    "> | Component       | Specification                |\n",
    "> |-----------------|------------------------------|\n",
    "> | **Activation**  | ReLU (Rectified Linear Unit) |\n",
    "> | **Optimizer**   | RMSProp                      |\n",
    "> | **Learning**    | Experience Replay + Target Network |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data\n",
    "tf_batch_dataset = tf.placeholder(tf.float32,shape=(batch_size,4,4,16))\n",
    "tf_batch_labels  = tf.placeholder(tf.float32,shape=(batch_size,output_units))\n",
    "\n",
    "single_dataset   = tf.placeholder(tf.float32,shape=(1,4,4,16))\n",
    "\n",
    "# conv layer1 weights\n",
    "conv1_layer1_weights = tf.Variable(tf.truncated_normal([1,2,input_units,depth1],mean=0,stddev=0.01))\n",
    "conv2_layer1_weights = tf.Variable(tf.truncated_normal([2,1,input_units,depth1],mean=0,stddev=0.01))\n",
    "\n",
    "# conv layer2 weights\n",
    "conv1_layer2_weights = tf.Variable(tf.truncated_normal([1,2,depth1,depth2],mean=0,stddev=0.01))\n",
    "conv2_layer2_weights = tf.Variable(tf.truncated_normal([2,1,depth1,depth2],mean=0,stddev=0.01))\n",
    "\n",
    "# FUllY CONNECTED LAYERS\n",
    "expand_size = 2*4*depth2*2 + 3*3*depth2*2 + 4*3*depth1*2\n",
    "fc_layer1_weights = tf.Variable(tf.truncated_normal([expand_size,hidden_units],mean=0,stddev=0.01))\n",
    "fc_layer1_biases  = tf.Variable(tf.truncated_normal([1,hidden_units],mean=0,stddev=0.01))\n",
    "fc_layer2_weights = tf.Variable(tf.truncated_normal([hidden_units,output_units],mean=0,stddev=0.01))\n",
    "fc_layer2_biases  = tf.Variable(tf.truncated_normal([1,output_units],mean=0,stddev=0.01))\n",
    "\n",
    "def model(dataset):\n",
    "    # layer1\n",
    "    conv1 = tf.nn.conv2d(dataset,conv1_layer1_weights,[1,1,1,1],padding='VALID') \n",
    "    conv2 = tf.nn.conv2d(dataset,conv2_layer1_weights,[1,1,1,1],padding='VALID') \n",
    "    \n",
    "    # layer1 relu activation\n",
    "    relu1 = tf.nn.relu(conv1)\n",
    "    relu2 = tf.nn.relu(conv2)\n",
    "    \n",
    "    # layer2\n",
    "    conv11 = tf.nn.conv2d(relu1,conv1_layer2_weights,[1,1,1,1],padding='VALID') \n",
    "    conv12 = tf.nn.conv2d(relu1,conv2_layer2_weights,[1,1,1,1],padding='VALID') \n",
    "    \n",
    "    conv21 = tf.nn.conv2d(relu2,conv1_layer2_weights,[1,1,1,1],padding='VALID') \n",
    "    conv22 = tf.nn.conv2d(relu2,conv2_layer2_weights,[1,1,1,1],padding='VALID') \n",
    "    \n",
    "    # layer2 relu activation\n",
    "    relu11 = tf.nn.relu(conv11)\n",
    "    relu12 = tf.nn.relu(conv12)\n",
    "    relu21 = tf.nn.relu(conv21)\n",
    "    relu22 = tf.nn.relu(conv22)\n",
    "    \n",
    "    # get shapes of all activations\n",
    "    shape1 = relu1.get_shape().as_list()\n",
    "    shape2 = relu2.get_shape().as_list()\n",
    "    \n",
    "    shape11 = relu11.get_shape().as_list()\n",
    "    shape12 = relu12.get_shape().as_list()\n",
    "    shape21 = relu21.get_shape().as_list()\n",
    "    shape22 = relu22.get_shape().as_list()\n",
    "    \n",
    "    # expansion\n",
    "    hidden1  = tf.reshape(relu1,[shape1[0],shape1[1]*shape1[2]*shape1[3]])\n",
    "    hidden2  = tf.reshape(relu2,[shape2[0],shape2[1]*shape2[2]*shape2[3]])\n",
    "    \n",
    "    hidden11 = tf.reshape(relu11,[shape11[0],shape11[1]*shape11[2]*shape11[3]])\n",
    "    hidden12 = tf.reshape(relu12,[shape12[0],shape12[1]*shape12[2]*shape12[3]])\n",
    "    hidden21 = tf.reshape(relu21,[shape21[0],shape21[1]*shape21[2]*shape21[3]])\n",
    "    hidden22 = tf.reshape(relu22,[shape22[0],shape22[1]*shape22[2]*shape22[3]])\n",
    "    \n",
    "    # concatenation\n",
    "    hidden = tf.concat([hidden1,hidden2,hidden11,hidden12,hidden21,hidden22],axis=1)\n",
    "    \n",
    "    # full connected layers\n",
    "    hidden = tf.matmul(hidden,fc_layer1_weights) + fc_layer1_biases\n",
    "    hidden = tf.nn.relu(hidden)\n",
    "    \n",
    "    # output layer\n",
    "    output = tf.matmul(hidden,fc_layer2_weights) + fc_layer2_biases\n",
    "    \n",
    "    return output\n",
    "\n",
    "# for single example\n",
    "single_output = model(single_dataset)\n",
    "\n",
    "# for batch data\n",
    "logits = model(tf_batch_dataset)\n",
    "\n",
    "# loss\n",
    "loss = tf.square(tf.subtract(tf_batch_labels,logits))\n",
    "loss = tf.reduce_sum(loss,axis=1,keepdims=True)\n",
    "loss = tf.reduce_mean(loss)/2.0\n",
    "\n",
    "# optimizer\n",
    "global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "learning_rate = tf.train.exponential_decay(float(start_learning_rate), global_step, 1000, 0.90, staircase=True)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\"><strong>Create training dataset and Train Simultaneously</strong></h3>\n",
    "\n",
    "* `Current Reward` = `number of merges` + ``log(new max,2)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = []                     # loss\n",
    "scores = []                # score\n",
    "final_parameters = {}      # to store final parameters w/ highest score\n",
    "M = 101                    # number of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    global epsilon\n",
    "    global replay_labels\n",
    "    global replay_memory\n",
    "    \n",
    "    # for episode with max score\n",
    "    maximum = -1\n",
    "    episode = -1\n",
    "    \n",
    "    # total_iters \n",
    "    total_iters = 1\n",
    "    \n",
    "    # number of back props\n",
    "    back=0\n",
    "    \n",
    "    for ep in range(M):\n",
    "        global board\n",
    "        board = new_game(4)\n",
    "        add_two(board)\n",
    "        add_two(board)\n",
    "        \n",
    "        # whether episode finished or not\n",
    "        finish = 'not over'\n",
    "        \n",
    "        # total_score of this episode\n",
    "        total_score = 0\n",
    "        \n",
    "        # iters per episode\n",
    "        local_iters = 1\n",
    "        \n",
    "        while(finish=='not over'):\n",
    "            prev_board = deepcopy(board)\n",
    "            \n",
    "            # get the required move for this state\n",
    "            state = deepcopy(board)\n",
    "            state = change_values(state)\n",
    "            state = np.array(state,dtype = np.float32).reshape(1,4,4,16)\n",
    "            feed_dict = {single_dataset:state}\n",
    "            control_scores = session.run(single_output,feed_dict=feed_dict)\n",
    "            \n",
    "            # find the move with max Q value\n",
    "            control_buttons = np.flip(np.argsort(control_scores),axis=1)\n",
    "            \n",
    "            # copy the Q-values as labels\n",
    "            labels = deepcopy(control_scores[0])\n",
    "            \n",
    "            # generate random number for epsilon greedy approach\n",
    "            num = random.uniform(0,1)\n",
    "            \n",
    "            # store prev max\n",
    "            prev_max = np.max(prev_board)\n",
    "            \n",
    "            # num is less epsilon generate random move\n",
    "            if(num<epsilon):\n",
    "                #find legal moves\n",
    "                legal_moves = list()\n",
    "                for i in range(4):\n",
    "                    temp_board = deepcopy(prev_board)\n",
    "                    temp_board,_,_ = controls[i](temp_board)\n",
    "                    if(np.array_equal(temp_board,prev_board)):\n",
    "                        continue\n",
    "                    else:\n",
    "                        legal_moves.append(i)\n",
    "                if(len(legal_moves)==0):\n",
    "                    finish = 'lose'\n",
    "                    continue\n",
    "                \n",
    "                # generate random move.\n",
    "                con = random.sample(legal_moves,1)[0]\n",
    "                \n",
    "                # apply the move\n",
    "                temp_state = deepcopy(prev_board)\n",
    "                temp_state,_,score = controls[con](temp_state)\n",
    "                total_score += score\n",
    "                finish = game_state(temp_state)\n",
    "                \n",
    "                # get number of merges\n",
    "                empty1 = findemptyCell(prev_board)\n",
    "                empty2 = findemptyCell(temp_state)\n",
    "                \n",
    "                if(finish=='not over'):\n",
    "                    temp_state = add_two(temp_state)\n",
    "                \n",
    "                board = deepcopy(temp_state)\n",
    "                \n",
    "                # get next max after applying the move\n",
    "                next_max = np.max(temp_state)\n",
    "                \n",
    "                # reward math.log(next_max,2)*0.1 if next_max is higher than prev max\n",
    "                labels[con] = (math.log(next_max,2))*0.1\n",
    "                \n",
    "                if(next_max==prev_max):\n",
    "                    labels[con] = 0\n",
    "                \n",
    "                # reward is also the number of merges\n",
    "                labels[con] += (empty2-empty1)\n",
    "                \n",
    "                # get the next state max Q-value\n",
    "                temp_state = change_values(temp_state)\n",
    "                temp_state = np.array(temp_state,dtype = np.float32).reshape(1,4,4,16)\n",
    "                feed_dict = {single_dataset:temp_state}\n",
    "                temp_scores = session.run(single_output,feed_dict=feed_dict)\n",
    "                    \n",
    "                max_qvalue = np.max(temp_scores)\n",
    "                \n",
    "                # final labels add gamma*max_qvalue\n",
    "                labels[con] = (labels[con] + gamma*max_qvalue)\n",
    "            \n",
    "            # generate the the max predicted move\n",
    "            else:\n",
    "                for con in control_buttons[0]:\n",
    "                    prev_state = deepcopy(prev_board)\n",
    "                    \n",
    "                    # apply the LEGAl Move with max q_value\n",
    "                    temp_state,_,score = controls[con](prev_state)\n",
    "                    \n",
    "                    # if illegal move label = 0\n",
    "                    if(np.array_equal(prev_board,temp_state)):\n",
    "                        labels[con] = 0\n",
    "                        continue\n",
    "                    \n",
    "                    # get number of merges\n",
    "                    empty1 = findemptyCell(prev_board)\n",
    "                    empty2 = findemptyCell(temp_state)\n",
    "                    \n",
    "                    temp_state = add_two(temp_state)\n",
    "                    board = deepcopy(temp_state)\n",
    "                    total_score += score\n",
    "                    next_max = np.max(temp_state)\n",
    "                    \n",
    "                    # reward\n",
    "                    labels[con] = (math.log(next_max,2))*0.1\n",
    "                    if(next_max==prev_max):\n",
    "                        labels[con] = 0\n",
    "                    \n",
    "                    labels[con] += (empty2-empty1)\n",
    "                    \n",
    "                    # get next max qvalue\n",
    "                    temp_state  = change_values(temp_state)\n",
    "                    temp_state  = np.array(temp_state,dtype = np.float32).reshape(1,4,4,16)\n",
    "                    feed_dict   = {single_dataset:temp_state}\n",
    "                    temp_scores = session.run(single_output,feed_dict=feed_dict)\n",
    "                    max_qvalue  = np.max(temp_scores)\n",
    "                    \n",
    "                    # final labels\n",
    "                    labels[con] = (labels[con] + gamma*max_qvalue)\n",
    "                    break\n",
    "                    \n",
    "                if(np.array_equal(prev_board,board)):\n",
    "                    finish = 'lose'\n",
    "            \n",
    "            # decrease the epsilon value\n",
    "            if((ep>10000) or (epsilon>0.1 and total_iters%2500==0)):\n",
    "                epsilon = epsilon/1.005\n",
    "            \n",
    "            # change the matrix values and store them in memory\n",
    "            prev_state = deepcopy(prev_board)\n",
    "            prev_state = change_values(prev_state)\n",
    "            prev_state = np.array(prev_state,dtype=np.float32).reshape(1,4,4,16)\n",
    "            replay_labels.append(labels)\n",
    "            replay_memory.append(prev_state)\n",
    "            \n",
    "            # back-propagation\n",
    "            if(len(replay_memory)>=mem_capacity):\n",
    "                back_loss = 0\n",
    "                batch_num = 0\n",
    "                z = list(zip(replay_memory,replay_labels))\n",
    "                np.random.shuffle(z)\n",
    "                np.random.shuffle(z)\n",
    "                replay_memory,replay_labels = zip(*z)\n",
    "                \n",
    "                for i in range(0,len(replay_memory),batch_size):\n",
    "                    if(i + batch_size>len(replay_memory)):\n",
    "                        break\n",
    "                    \n",
    "                    batch_data   = deepcopy(replay_memory[i:i+batch_size])\n",
    "                    batch_labels = deepcopy(replay_labels[i:i+batch_size])\n",
    "                    \n",
    "                    batch_data   = np.array(batch_data,dtype=np.float32).reshape(batch_size,4,4,16)\n",
    "                    batch_labels = np.array(batch_labels,dtype=np.float32).reshape(batch_size,output_units)\n",
    "                    feed_dict = {tf_batch_dataset: batch_data, tf_batch_labels: batch_labels}\n",
    "                    _,l = session.run([optimizer,loss],feed_dict=feed_dict)\n",
    "                    back_loss += l \n",
    "                    \n",
    "                    print(\"Mini-Batch - {} Back-Prop : {}, Loss : {}\".format(batch_num,back,l))\n",
    "                    batch_num +=1\n",
    "                back_loss /= batch_num\n",
    "                J.append(back_loss)\n",
    "                \n",
    "                # store the parameters in a dictionary\n",
    "                final_parameters['conv1_layer1_weights'] = session.run(conv1_layer1_weights)\n",
    "                final_parameters['conv1_layer2_weights'] = session.run(conv1_layer2_weights)\n",
    "                final_parameters['conv2_layer1_weights'] = session.run(conv2_layer1_weights)\n",
    "                final_parameters['conv2_layer2_weights'] = session.run(conv2_layer2_weights)\n",
    "                final_parameters['fc_layer1_weights'] = session.run(fc_layer1_weights)\n",
    "                final_parameters['fc_layer2_weights'] = session.run(fc_layer2_weights)\n",
    "                final_parameters['fc_layer1_biases']  = session.run(fc_layer1_biases)\n",
    "                final_parameters['fc_layer2_biases']  = session.run(fc_layer2_biases)\n",
    "                \n",
    "                # number of back-props\n",
    "                back+=1\n",
    "                \n",
    "                # make new memory \n",
    "                replay_memory = list()\n",
    "                replay_labels = list()\n",
    "            \n",
    "            if(local_iters%400==0):\n",
    "                print(\"Episode : {}, Score : {}, Iters : {}, Finish : {}\".format(ep,total_score,local_iters,finish))\n",
    "            \n",
    "            local_iters += 1\n",
    "            total_iters += 1\n",
    "            \n",
    "        scores.append(total_score)\n",
    "        print(\"Episode {} finished with score {}, result : {} board : {}, epsilon  : {}, learning rate : {} \".format(ep,total_score,finish,board,epsilon,session.run(learning_rate)))\n",
    "        print()\n",
    "        \n",
    "        if((ep+1)%1000==0):\n",
    "            print(\"Maximum Score : {} ,Episode : {}\".format(maximum,episode))    \n",
    "            print(\"Loss : {}\".format(J[len(J)-1]))\n",
    "            print()\n",
    "        \n",
    "        if(maximum<total_score):\n",
    "            maximum = total_score\n",
    "            episode = ep\n",
    "    print(\"Maximum Score : {} ,Episode : {}\".format(maximum,episode))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Check train and all episodes are well done\n",
    "print(f\"Number of episodes : {scores}\")\n",
    "print(f\"Max score          : {max(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\"><strong>Store the Trained Weights in a file</strong></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./model_weights\"\n",
    "weights = ['conv1_layer1_weights','conv1_layer2_weights','conv2_layer1_weights','conv2_layer2_weights','fc_layer1_weights','fc_layer1_biases','fc_layer2_weights','fc_layer2_biases']\n",
    "for w in weights:\n",
    "    flatten = final_parameters[w].reshape(-1,1)\n",
    "    file = open(path + '\\\\' + w +'.csv','w')\n",
    "    file.write('Sno,Weight\\n')\n",
    "    for i in range(flatten.shape[0]):\n",
    "        file.write(str(i) +',' +str(flatten[i][0])+'\\n') \n",
    "    file.close()\n",
    "    print(w + \"\\twritten!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Team Members:**\n",
    "<div style=\"background: linear-gradient(135deg,rgb(70, 104, 197),rgb(190, 144, 149)); \n",
    "            color: #ffffff; \n",
    "            width: 100%; \n",
    "            padding: 20px 0;\n",
    "            text-align: left; \n",
    "            font-weight: bold; \n",
    "            margin: 15px 0; \n",
    "            font-size: 24px; \n",
    "            border-radius: 10px; \n",
    "            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);\">\n",
    "    <div style=\"margin: 10px 0;\">1️⃣ Hassan Hamed Zoghly</div>\n",
    "    <div style=\"margin: 10px 0;\">2️⃣ Abdelrhman Ahmed Ezzat</div>\n",
    "    <div style=\"margin: 10px 0;\">3️⃣ Ahmed Abdullateif Amer</div>\n",
    "    <div style=\"margin: 10px 0;\">4️⃣ Mostafa Nasser Fouda</div>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
